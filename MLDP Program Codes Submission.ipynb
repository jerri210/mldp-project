{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7fa6d7f",
   "metadata": {},
   "source": [
    "# Declaration of Originality"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "58ad86aa",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)\n",
    "\n",
    "**TEMASEK POLYTECHNIC**\n",
    "<br>**SCHOOL OF INFORMATICS & IT**\n",
    "<br>**DIPLOMA IN INFORMATION TECHNOLOGY**\n",
    "<br>**MACHINE LEARNING FOR DEVELOPERS (CAI2C08)**\n",
    "<br>**AY2025/2026 OCTOBER SEMESTER**\n",
    "\n",
    "**PROJECT PROGRAM CODES**\n",
    "* Student Name (Matric Number)  : Jerrica Low Yuin En (2404802G)\n",
    "* Tutorial Group                : T07\n",
    "* Tutor\t\t\t\t\t\t    : Ms Ester\n",
    "* Submission Date               : 11 Feb 2026\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26f1aba",
   "metadata": {},
   "source": [
    "**Declaration of Originality**\n",
    "* I am the originator of this work and I have appropriately acknowledged all other original sources used as my references for this work.\n",
    "* I understand that Plagiarism is the act of taking and using the whole or any part of another personâ€™s work, including work generated by AI, and presenting it as my own.\n",
    "* I understand that Plagiarism is an academic offence and if I am found to have committed or abetted the offence of plagiarism in relation to this submitted work, disciplinary action will be enforced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad90912",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85eed7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To import all the required libraries throughout this porject\n",
    "\n",
    "# Data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Models (regression)\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV,KFold\n",
    "\n",
    "# Regression metrics\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44844094",
   "metadata": {},
   "source": [
    "# 1. Business Understanding\n",
    "Goal: The goal of this project is to develop a machine learning model that is able to predict the price of a car based on its features, such as the vehicle specification, usage, and other relevant attributes.\n",
    "\n",
    "Being able to accurately predict the car prices can help sellers to set an apporpriate competitive prices and assist buyers in making an informed purcharshing decisions beforehand. \n",
    "\n",
    "Problem Type: This problem is a regression task, as the target variable which is \"car price\" is a continuous numerical value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6f9f54",
   "metadata": {},
   "source": [
    "# 2. Data Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af93e64c",
   "metadata": {},
   "source": [
    "## 2.1 Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2591ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load my dataset from the CSV file into the pandas DataFrame to get a look at the data\n",
    "FILE_PATH= \"car_price_prediction.csv\"\n",
    "df = pd.read_csv(FILE_PATH)\n",
    "\n",
    "# To view the first five row of my dataset \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b1d880",
   "metadata": {},
   "source": [
    "## 2.2 Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f52a2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understand the type of variable for each column\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92177b5",
   "metadata": {},
   "source": [
    "Insights: \n",
    "\n",
    "Currently, this dataset contains 19237 rows and 18 columns, which has a mix of numerical and categorical features. For the numerical (int64) there are 5 columns which includes the Price, Prod. year, Cylinders, Airbags and ID, while the remaining 13 columns are categorical (object).\n",
    "\n",
    "The target variable which is the \"Price\" is numberical, which helps to confirm that this project is a regression task. But with multiple categorical variables it does indicate that feature encoding would be needed before training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9eb4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing data\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68eacba0",
   "metadata": {},
   "source": [
    "Insights:\n",
    "\n",
    "There are no missing values in this dataset, which means all of 18 columns does contain complete data, which also includes the target variable \"Price\". So no imputation or removal of records is required for any missing value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5f5189",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Describe data distribution\n",
    "df.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a914f7",
   "metadata": {},
   "source": [
    "Insights:\n",
    "\n",
    "This summary statistic does show some NaN values for certain metics, but they only appear as some statistic aren't applicable to the specific data type. \n",
    "\n",
    "Such as some numerical statistic such as the mean and standard deviation tend to only compute for the numerical columns, while the categorical statistic such as the unique values, as well as most frequent categoris are only computed for categorical columns. \n",
    "\n",
    "But this does confirm that the dataset contains a mix of numerical and categorical features, while will need the appropriate preprocessing later before training the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76dcd583",
   "metadata": {},
   "source": [
    "## 2.3 Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b08d42",
   "metadata": {},
   "source": [
    "### 2.3.1 Understanding distribution of data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66369e79",
   "metadata": {},
   "source": [
    "### 2.3.1.1 Understanding distribution of target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4a4b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Understanding distribution of target\n",
    "col_y = \"Price\"  \n",
    "\n",
    "# Summary statistics of target variable\n",
    "df[col_y].describe()\n",
    "\n",
    "# Histogram of target variable\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.histplot(df[col_y], bins=30, kde=True)\n",
    "plt.title(\"Distribution of Car Prices\")\n",
    "plt.xlabel(\"Car Price\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Boxplot\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.boxplot(x=df[col_y])\n",
    "plt.title(\"Boxplot of Car Prices\")\n",
    "plt.xlabel(\"Car Price\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Log-transformed target distribution - histogram\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.histplot(np.log1p(df[col_y]), bins=30, kde=True)\n",
    "plt.title(\"Log-Transformed Distribution of Car Prices\")\n",
    "plt.xlabel(\"Log(Car Price + 1)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102d4af8",
   "metadata": {},
   "source": [
    "Insights:\n",
    "\n",
    "- Histogram of the Car Prices\n",
    "\n",
    "For the historgram of the car prices it does show that it is highly right-skewed distribution. Which means most car prices are concentrated in the lower price range, while there is only a small number of car that are extremely high priced. The long right tail indicates that the presence of luxury or the rare vehicles are prices significantly higher than most of the cars in this dataset \n",
    "\n",
    "But this could be quite common in the real world pricing data and does negatively impact the model performance, especially for models that assumes a more symmetic distribution. As well as this large range of the values might also suggest that high prices outliers may disproportionately influence the model when training. \n",
    "\n",
    "- Boxplot of the Car Prices \n",
    "\n",
    "The boxplot does help to confirm that there is definetly many outliers in the car price data. As most of the observateion are grouped quite tightly near the lower end of the price range, while only some points are laying far above the upper whisker. \n",
    "\n",
    "These outliers represent the unusually expensive vehicles and identifying these outliers are important as they could distort my error metrics such as my RMSE and it could cause the model to focus more on the rare cases, as well as reducing the model ability to generalize to typical car prices, so this helps me in deciding whether i need to transform the target variable or not. \n",
    "\n",
    "- Log Transformed Distribution of Car Prices\n",
    "\n",
    "After I had applied the logarithmic transformation to the car prices, the distribution has becom emore symmetric and more bell-shaped. So the extreme skewness is reduced and the influence of the larger value are compressed. \n",
    "\n",
    "So this help to stabilizes the variance, reducing the impact of the extreme outliers, allowing the model to learn the patterns more effectively and improving the covergence and predictive performance for many regression models, so as a result, a log transformed target variable is more suite for modelling and would expect a better performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcab4c87",
   "metadata": {},
   "source": [
    "### 2.3.1.2 Understanding distribution of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97aba5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove ID column (only the identifier)\n",
    "if \"ID\" in df.columns:\n",
    "    df = df.drop(columns=[\"ID\"])\n",
    "\n",
    "# Numerical features distribution\n",
    "# To select the numerical features and exclude target variable\n",
    "num_features = df.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
    "num_features = num_features.drop(col_y)\n",
    "\n",
    "# Plot histograms for numerical features\n",
    "df[num_features].hist(figsize=(12, 8))\n",
    "plt.suptitle(\"Distribution of Numerical Features\", y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1656c4bb",
   "metadata": {},
   "source": [
    "Insights:\n",
    "\n",
    "This numerical features has showed the varied distribution across the dataset. \n",
    "\n",
    "For the Production year it is right-skewed, with most of the vehicle being manufactures after 2000, which indicates that the newer cars does dominate the dataset, While the Cylinders are concentrated to around 4-6, it suggest that most of the vehicle are using the standard engine configurations, while there are some with extreme values that are rare. But for the Airbags it displays a discrete distribution with peaks at the safety configurations, showing the standard manufacturer designs instead of the continuous variation. \n",
    "\n",
    "Overall, these distribution indicated that the numerical features aren't normally distributed and they do contain skewnedd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccbdba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical features distribution\n",
    "cat_features = df.select_dtypes(include=[\"object\", \"category\"]).columns\n",
    "\n",
    "# To plot the top 10 categories for each categorical feature\n",
    "for col in cat_features:\n",
    "    plt.figure(figsize=(7, 4))\n",
    "    df[col].value_counts().head(10).plot(kind=\"bar\")\n",
    "    plt.title(f\"Top 10 Categories in {col}\")\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85390709",
   "metadata": {},
   "source": [
    "Insights:\n",
    "\n",
    "This categorical features has showed the varied distribution across the dataset. \n",
    "\n",
    "This was done to help me understand the frequency and alance of the categorical variaible within my dataset, its important as it helps with indetifying any domainant categories, rare values, and feature with limited variability, which could affect how well the machine learning model learns the pattern after encoding. \n",
    "By being able to vissualise the top categories for each of the feature, it help me to make a better informed decision on wheter to retain, group or remove and certain categorical variables before the modelling step. \n",
    "\n",
    "For the Category feature, the dataset is heavily domiated by Sedan, Jeep, and Hatchback vehicle types, while the categories such as Pickup, Cabriolet and Goods wagon appear very infrequently. So this indicates that there is a strong class imbalance withing the vehicle body types, where there is a small number of categories account for the majority of observations. \n",
    "\n",
    "For the Leather interior feature, it contains only two categories which is \"Yes\" and \"No\", but the vehicles that has the leather interiors appears more frequently. So this somewhat simple distribution suggests that the leather interiror is a more common characteristic within the dataset and may affect the difference in the vehicle quality or the pricing. \n",
    "\n",
    "For the Fuel type feature, it shows that there is a large proportion of Petrol vehicle in the dataset, followed by the Dieel and Hybrid vehicles. While in contrat, LPG,CNG, Plug in Hybrid and Hydrogen vehicles only occurs in small numbers. So this does highlight that the skewed distribution where alternative fuel type are underrepresented.\n",
    "\n",
    "For the Gear box type feature, the Automatic transmission is the most prevalent, with the Tiptronic and the Manual transmissing appearing less frequently, while the Variator transmissing is also the least commong one. Hence, the distribution does reflect that there is a strong prefernece towards automatic transmission in the dataset.\n",
    "\n",
    "For the Drive wheel feature, it is also imbalanced, with the Front-wheel drive(fwd) being the most common configuration, while the Rear-wheel drive and the All-wheel drive(awd) does appear less frequently. But despite the imbalance, there are multiple drivetrain (rwd) configuration that are in the dataset.\n",
    "\n",
    "For the Wheel feature, it does show that majority of the vehicles are left-hand drive, while there is only a small portion of right-hand drive vehicles. Which indicates that there are limited variability in the steeering orientation. \n",
    "\n",
    "Lastly, the Doors_group feature, it shows that most of the vehicles have four or more doors (le_4), while vehicles that has lesser doors (gt_4) are more rare. So this does suggest that the dataset is heavily skewed towards the standard multi-door vehicles.\n",
    "\n",
    "In conclusion, this analysis does show that a few of the variables are imbalances and have limited variation. Which helps provide me with a clearer understanding of the dataset to help with the later part of my data cleaning and feature engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6555bda5",
   "metadata": {},
   "source": [
    "### 2.3.2 Understanding relationship between variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82056dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Understanding relationship between variables\n",
    "\n",
    "# Select numerical columns only\n",
    "num_cols = df.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
    "\n",
    "# Correlation heatmap for numerical features\n",
    "plt.figure(figsize=(10, 8))\n",
    "corr = df[num_cols].corr()\n",
    "\n",
    "sns.heatmap(corr, annot=True, cmap=\"coolwarm\", linewidths=0.5)\n",
    "plt.title(\"Correlation Heatmap of Numerical Features\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Pairplot for selected important numerical features\n",
    "selected_features = [col_y] + list(\n",
    "    df.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
    "    .drop(col_y)\n",
    "    .sort_values()[:3]   # take a few numerical features only\n",
    ")\n",
    "\n",
    "sns.pairplot(df[num_cols])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66d4b3b",
   "metadata": {},
   "source": [
    "Insights: \n",
    "\n",
    "The correlation heatmap (the first visual), it shows that the Pirce has a very weak correlations with the Production year, Cylinders, and Airbags, with the values being close to zero. Which indicates that the vehicle price dosent have a strong relationship with any of the single numerical variable. Although there is a little positive relationship between the Production year and the Airbags, which may suggest that the newer vehicles does have more safety features. \n",
    "\n",
    "While the pairplot (the second value), help to further support the finding as the scatter plots involving the Price does show a widely dispered points with no clear linear trend at all. As well as several numerical varibale does exhibit discrete values, which results in clustered patters rather than any continuous relationship.\n",
    "\n",
    "Hence, this analysis does show there is a weak linear relationship between the numerical features and the Price, which shows that the Price might be affected by other multiple factors instead of just a single numerical attribute."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a426691",
   "metadata": {},
   "source": [
    "# 3. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3517108",
   "metadata": {},
   "source": [
    "## 3.1 Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9949bb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data preprocessing - to drop any unnecessary columns\n",
    "print(\"Initial dataset shape\")\n",
    "print(df.shape)\n",
    "\n",
    "print(\"\\nData types\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# Drop columns that are not required for modelling\n",
    "drop_cols = [\"ID\", \"Manufacturer\", \"Model\", \"Color\"]\n",
    "drop_exist = [c for c in drop_cols if c in df.columns]\n",
    "df = df.drop(columns=drop_exist)\n",
    "\n",
    "print(\"\\nDropped columns\")\n",
    "print(drop_exist)\n",
    "\n",
    "print(\"\\nRemaining columns\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "print(\"\\nDataset shape after dropping columns\")\n",
    "print(df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0132260",
   "metadata": {},
   "source": [
    "Insights:\n",
    "\n",
    "I have reviews the dataset structure, which includes its shape and the data types, so currently the dataset has 19237 records and 17 columns, which consist of both numerical and categorical features, but is 1 column short as I have previously dropped the \"ID\" column. \n",
    "\n",
    "The columns that aren't relevant are Manufacturer, Model and Color, as well as dropping \"ID\" again incase the top hasn't dropped it successfully yet, as none of this 4 columns provide any predictive value for the vehicle price, which they are either just descriptive or only used as a identifiers. \n",
    "\n",
    "After removing the columns, the current dataset has only 14 relevant features while reaining all the records. So this help with simplifying the dataset and ensuring that only attributes that are meaningful are kept for the preprocessing and modelling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8384d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_numeric_col(series):\n",
    "    s = series.astype(str).str.replace(\",\", \"\", regex=False).str.strip()\n",
    "    s = s.replace([\"-\", \"nan\", \"None\", \"\"], np.nan)\n",
    "    return pd.to_numeric(s, errors=\"coerce\")\n",
    "\n",
    "\n",
    "def boxplot_series(series, title, xlabel):\n",
    "    plt.figure(figsize=(7, 3))\n",
    "    plt.boxplot(series.dropna(), vert=False)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def cap_outliers_iqr(series, k):\n",
    "    q1 = series.quantile(0.25)\n",
    "    q3 = series.quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "\n",
    "    low = q1 - k * iqr\n",
    "    high = q3 + k * iqr\n",
    "\n",
    "    capped = series.clip(lower=low, upper=high)\n",
    "    return capped, low, high\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3684f5",
   "metadata": {},
   "source": [
    "This portion is to define the helper function that will be used for below, which includes a function to clean and convert the numeric columns stored as text, a function to help with visuallising the numerical distribution using boxplot, and a function to handle the outlier using the IQR method by capping the extreme values. \n",
    "\n",
    "So all this can help ensure that my numerical features are clean, consistent and would be suitable for modelling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7884132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure Price is numeric\n",
    "df[\"Price\"] = clean_numeric_col(df[\"Price\"])\n",
    "\n",
    "print(\"Missing values in Price\")\n",
    "print(df[\"Price\"].isna().sum())\n",
    "\n",
    "before = len(df)\n",
    "df = df.dropna(subset=[\"Price\"])\n",
    "print(\"Rows dropped due to missing Price\")\n",
    "print(before - len(df))\n",
    "\n",
    "print(\"Price statistics before outlier handling\")\n",
    "print(df[\"Price\"].describe())\n",
    "\n",
    "boxplot_series(df[\"Price\"], \"Boxplot of car prices before outlier handling\", \"Price\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2e3d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle outliers using IQR capping\n",
    "k = 1.0\n",
    "df[\"Price_clean\"], low, high = cap_outliers_iqr(df[\"Price\"], k)\n",
    "\n",
    "print(\"Outlier capping limits for Price\")\n",
    "print(\"Lower cap\")\n",
    "print(low)\n",
    "print(\"Upper cap\")\n",
    "print(high)\n",
    "\n",
    "print(\"Price statistics after outlier handling\")\n",
    "print(df[\"Price_clean\"].describe())\n",
    "\n",
    "boxplot_series(df[\"Price_clean\"], \"Boxplot of car prices after outlier handling\", \"Price cleaned\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633b0786",
   "metadata": {},
   "source": [
    "Insights:\n",
    "\n",
    "Before the outlier handling, the Price feature was analysed using the summary statistic and a boxplot, and though there wasn't any missing values, the distribution does show that there is an extreme price values, with a huge range between the minimum and the maximum prices. Hence the boxplot shows that there several high end outliers, which indicates that it is highly skewed price distribution that could negatively affect the model performance. \n",
    "\n",
    "As there is extreme values, I have decided to use the Interquartile Range (IQR) method to handle the outliers, instead of removing the obeservation, the outliers have been capped within the deined lower and upper bound. So that I am able to preserve all teh data point while reducing the influence of the extreme price values on the model.\n",
    "\n",
    "After I had applied the IQR-based capping, the price distribution became more balanced, which the extreme values were now limited, so that the distribution is more stable and better suited for the machine learning models and helping with improving the robustness of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3c1f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Levy\n",
    "print(\"Levy:\")\n",
    "\n",
    "# Convert the Levy to numeric FIRST\n",
    "df[\"Levy_raw\"] = clean_numeric_col(df[\"Levy\"])\n",
    "\n",
    "# BEFORE filling missing values\n",
    "print(\"Levy statistics BEFORE filling missing values\")\n",
    "print(df[\"Levy_raw\"].describe())\n",
    "\n",
    "boxplot_series(df[\"Levy_raw\"], \"Boxplot of Levy (Before Imputation)\", \"Levy\")\n",
    "\n",
    "# Fill missing values\n",
    "df[\"Levy\"] = df[\"Levy_raw\"].fillna(df[\"Levy_raw\"].median())\n",
    "\n",
    "# AFTER cleaning\n",
    "print(\"Levy statistics AFTER cleaning\")\n",
    "print(df[\"Levy\"].describe())\n",
    "\n",
    "boxplot_series(df[\"Levy\"], \"Boxplot of Levy (After Cleaning)\", \"Levy\")\n",
    "\n",
    "# drop the helper column\n",
    "df = df.drop(columns=[\"Levy_raw\"])\n",
    "\n",
    "# Production year\n",
    "print(\"Production Year:\")\n",
    "\n",
    "# Before cleaning of the Production year\n",
    "print(\"Production year statistics BEFORE cleaning\")\n",
    "print(df[\"Prod. year\"].describe())\n",
    "\n",
    "boxplot_series(df[\"Prod. year\"], \"Boxplot of Production Year (Before Cleaning)\", \"Production year\")\n",
    "\n",
    "# Clean Production year\n",
    "df[\"Prod. year\"] = pd.to_numeric(df[\"Prod. year\"], errors=\"coerce\")\n",
    "df[\"Prod. year\"] = df[\"Prod. year\"].fillna(df[\"Prod. year\"].median())\n",
    "\n",
    "before = len(df)\n",
    "df = df[df[\"Prod. year\"] >= 1990]\n",
    "\n",
    "print(\"Rows removed with production year before 1990\")\n",
    "print(before - len(df))\n",
    "\n",
    "# After cleaning of the Production year\n",
    "print(\"Production year statistics AFTER cleaning\")\n",
    "print(df[\"Prod. year\"].describe())\n",
    "\n",
    "boxplot_series(df[\"Prod. year\"], \"Boxplot of Production Year (After Cleaning)\", \"Production year\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da9fa0c",
   "metadata": {},
   "source": [
    "Insights: \n",
    "\n",
    "For the Levy feature, intially it contains non-nuerical values such as the dashes \"-\", which has prevented it from being in the numerical analysis. Hence, I had converted the dashes \"-\" into missing values (NaN), after I had imputed these missing values using the median of the Levy value ensuring that the data completeness. After it was done processing, the Levy feature becomes fully numeric with no remaining missing values. Which the final statitcs and boxplots shows that the distribution remains right skewed, which tend to be expected for cost related attributes, so it is still suitable for modelling.\n",
    "\n",
    "While for the Production year feature, although the values were already numeric and there was no missing values, I had still included the number conversion and imputation steps for consistency and incase I had any data had been missed out. I had remove vehicles that were produced before 1990, as these entries were relatively fewer and less representaitve of the current vehicle market. So after I had made the filtering, the distribution was more compact and it focuses more on the recent years, so that it is also more relevant for the machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bada22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Engine volume cleaning\n",
    "print(\"\\nEngine volume changes:\")\n",
    "print(\"- Converted to all lowercase\")\n",
    "print(\"- Removed any 'turbo' text\")\n",
    "print(\"- Extracted the numeric part (e.g., 2.0 from '2.0 turbo')\")\n",
    "print(\"- Converted any of the numeric and filled missing values with median\")\n",
    "\n",
    "ev = df[\"Engine volume\"].astype(str).str.lower()\n",
    "ev = ev.str.replace(\"turbo\", \"\", regex=False)\n",
    "ev = ev.str.extract(r\"([\\d\\.]+)\")\n",
    "df[\"Engine volume\"] = pd.to_numeric(ev[0], errors=\"coerce\")\n",
    "df[\"Engine volume\"] = df[\"Engine volume\"].fillna(df[\"Engine volume\"].median())\n",
    "\n",
    "\n",
    "# Mileage cleaning\n",
    "print(\"\\nMileage changes:\")\n",
    "print(\"- Converted to all lowercase\")\n",
    "print(\"- Removed all the 'km' text and commas\")\n",
    "print(\"- Replaced any invalid values such as ('-', '') with NaN\")\n",
    "print(\"- Converted any of the numeric and filled missing values with median\")\n",
    "\n",
    "m = df[\"Mileage\"].astype(str).str.lower()\n",
    "m = m.str.replace(\"km\", \"\", regex=False).str.replace(\",\", \"\", regex=False).str.strip()\n",
    "m = m.replace([\"-\", \"nan\", \"None\", \"\"], np.nan)\n",
    "df[\"Mileage\"] = pd.to_numeric(m, errors=\"coerce\")\n",
    "df[\"Mileage\"] = df[\"Mileage\"].fillna(df[\"Mileage\"].median())\n",
    "\n",
    "# Cylinders\n",
    "print(\"\\nCylinders changes:\")\n",
    "print(\"- Converted all to numeric\")\n",
    "print(\"- Filled in any missing values with median\")\n",
    "\n",
    "df[\"Cylinders\"] = pd.to_numeric(df[\"Cylinders\"], errors=\"coerce\")\n",
    "df[\"Cylinders\"] = df[\"Cylinders\"].fillna(df[\"Cylinders\"].median())\n",
    "\n",
    "# Airbags\n",
    "print(\"\\nAirbags changes:\")\n",
    "print(\"- Converted all to numeric\")\n",
    "print(\"- Filled in any missing values with median\")\n",
    "\n",
    "df[\"Airbags\"] = pd.to_numeric(df[\"Airbags\"], errors=\"coerce\")\n",
    "df[\"Airbags\"] = df[\"Airbags\"].fillna(df[\"Airbags\"].median())\n",
    "\n",
    "# Final checking \n",
    "print(\"\\nMissing values after numeric cleaning (all the columns):\")\n",
    "print(df.isna().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7404fa36",
   "metadata": {},
   "source": [
    "Insights:\n",
    "\n",
    "For the Engine volume feature , there are values containing the text \"turbo\" were standardised by extracting the numeric component and converting the feature into  a numeric format. As well as, converting the feature into numeric format with missing values filled using a median. \n",
    "\n",
    "For the Mileage feature, the values were cleaned by removing any text, commas or invalid entries before doing the numeric conversion and median imputation. \n",
    "\n",
    "For the Cylinders and Airbags feature, the values were converted directly into a numeric format, and if there were any missing values they would be handles using the median. \n",
    "\n",
    "After all these steps, I had done a final check to ensure there are no more missing values across all the numerical features, so that they are prepared for the modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca76f040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the categorical columns\n",
    "cat_cols = df.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "\n",
    "print(\"Categorical columns\")\n",
    "print(cat_cols)\n",
    "\n",
    "print(\"Unique values per categorical column\")\n",
    "for c in cat_cols:\n",
    "    print(c, df[c].nunique())\n",
    "\n",
    "print(\"Top values per categorical column\")\n",
    "for c in cat_cols:\n",
    "    print(c)\n",
    "    print(df[c].value_counts().head())\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3af875",
   "metadata": {},
   "source": [
    "Insights:\n",
    "\n",
    "The categorical features that has been identified are Category, Leather interior, Fuel type, Gear box type, Drive wheels, Wheel, and Doors_group which are all that have a few distinct value and are suitable for categorical encoding. \n",
    "\n",
    "For the Category feature, currently Sedan, Jeep and Hatchback are the top 3, while other vehicle types tend to appear less frequently. While the Leather interior and Wheel features are considered binary, with the leather interiors and left-hand drive vehicles being more common. \n",
    "\n",
    "For the Fuel type, Petrol vehicles makes up for most of the majority, followed by Diesel and Hybrid, wheras the other fuel types occurs less frequently. While the Gear box type, the majority is Automatic transmission while Tiptronic and Manual appears lesser. \n",
    "\n",
    "As well as for the Drive wheels, mostly are front-wheel drive vehicles, and for the Door_group it shows that most of the vehicle have four or more doors. \n",
    "\n",
    "Hence, there is currently an imabalnce and limited variantion for most of these categorical features, which give me a better overview for feature encoding and what I need to handle when preparing my model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c442831b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis but numeric only\n",
    "num_df = df.select_dtypes(include=[\"number\"]).copy()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(num_df.corr(), annot=False, cmap=\"coolwarm\")\n",
    "plt.title(\"Correlation heatmap of numeric features\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Correlation with target\n",
    "target = \"Price_clean\"\n",
    "\n",
    "if target in num_df.columns:\n",
    "    print(\"Correlation with \" + target)\n",
    "    print(num_df.corr()[target].sort_values(ascending=False))\n",
    "else:\n",
    "    print(\"Target column not found\")\n",
    "    print(\"Available numeric columns\")\n",
    "    print(num_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64cd0466",
   "metadata": {},
   "source": [
    "Insights:\n",
    "\n",
    "Currently this is the correlation analysis is using a heatmap where the color intensity represent the strength and directions the relationship between these numerical features, where if the warmer color such as red to orange, it represents a stronger positive correlations while the cooler color such as white to blue would represent a weaker or negative correlations. \n",
    "\n",
    "But most of the feature pairs appears in cooler shades, which shows that they have a weak linear relationship and  which shows that most of the numerical features does have a weak linear relationship with each other which indicates that there is limited correlation among each features.\n",
    "\n",
    "But among all the features, Production year has one of the strongest postive correlation with the target variable, Price_clean, reflected by a lighter color (light blue) and a higher correlation value. But the other features such as Engine volume, Cylinders, Mileage, Levy and the Airbags does display a dark color which is value close to zero, indicating that it has a very weak relationship with the vehicle pricing. \n",
    "\n",
    "Hence, the heatmap does visually confirm that there is not a single numerical feature that can strongly determine the vehicle price, supporting the use of multivariate models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bceaa85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Category integer encoding\n",
    "print(\"\\nCategory - Before\")\n",
    "print(df[\"Category\"].astype(str).value_counts().head(10))\n",
    "\n",
    "cats = sorted(df[\"Category\"].dropna().astype(str).unique())\n",
    "cat_map = {v: i for i, v in enumerate(cats)}\n",
    "df[\"Category_encoded\"] = df[\"Category\"].astype(str).map(cat_map)\n",
    "\n",
    "print(\"\\nCategory Encoding Map - After\")\n",
    "print(dict(list(cat_map.items())[:10]))\n",
    "\n",
    "# Leather binary\n",
    "print(\"\\nLeather interior - Before\")\n",
    "print(df[\"Leather interior\"].astype(str).str.lower().str.strip().value_counts(dropna=False))\n",
    "\n",
    "leather = df[\"Leather interior\"].astype(str).str.lower().str.strip()\n",
    "df[\"Leather_binary\"] = leather.map({\"yes\": 1, \"no\": 0})\n",
    "df[\"Leather_binary\"] = df[\"Leather_binary\"].fillna(0).astype(int)\n",
    "\n",
    "print(\"\\nLeather interior - After\")\n",
    "print(df[\"Leather_binary\"].value_counts(dropna=False))\n",
    "\n",
    "# Wheel binary\n",
    "print(\"\\n Wheel - Before\")\n",
    "print(df[\"Wheel\"].astype(str).str.lower().str.strip().value_counts(dropna=False))\n",
    "\n",
    "wheel = df[\"Wheel\"].astype(str).str.lower().str.strip()\n",
    "df[\"Wheel_binary\"] = np.where(wheel.str.contains(\"left\", na=False), 0, np.nan)\n",
    "df[\"Wheel_binary\"] = np.where(wheel.str.contains(\"right\", na=False), 1, df[\"Wheel_binary\"])\n",
    "df[\"Wheel_binary\"] = df[\"Wheel_binary\"].fillna(df[\"Wheel_binary\"].mode().iloc[0]).astype(int)\n",
    "\n",
    "print(\"\\nWheel - After\")\n",
    "print(df[\"Wheel_binary\"].value_counts(dropna=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e570122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drive wheels standardisation\n",
    "print(\"\\nDrive wheels - Before\")\n",
    "print(df[\"Drive wheels\"].astype(str).str.lower().str.strip().value_counts(dropna=False))\n",
    "\n",
    "dw = df[\"Drive wheels\"].astype(str).str.lower().str.strip()\n",
    "df[\"Drive wheels\"] = dw.replace({\n",
    "    \"front\": \"fwd\",\n",
    "    \"rear\": \"rwd\",\n",
    "    \"4x4\": \"awd\",\n",
    "    \"4wd\": \"awd\",\n",
    "})\n",
    "\n",
    "print(\"\\nDrive wheels - After\")\n",
    "print(df[\"Drive wheels\"].value_counts(dropna=False))\n",
    "\n",
    "# Doors grouping\n",
    "print(\"\\nDoors_group - Before\")\n",
    "print(df[\"Doors\"].astype(str).value_counts().head(10))\n",
    "\n",
    "d = df[\"Doors\"].astype(str).str.extract(r\"(\\d+)\")\n",
    "df[\"Doors\"] = pd.to_numeric(d[0], errors=\"coerce\")\n",
    "\n",
    "if df[\"Doors\"].notna().any():\n",
    "    df[\"Doors\"] = df[\"Doors\"].fillna(df[\"Doors\"].mode().iloc[0])\n",
    "else:\n",
    "    df[\"Doors\"] = df[\"Doors\"].fillna(4)\n",
    "\n",
    "df[\"Doors_group\"] = np.where(df[\"Doors\"] <= 4, \"le_4\", \"gt_4\")\n",
    "\n",
    "print(\"\\nDoors_group - After\")\n",
    "print(df[\"Doors_group\"].value_counts(dropna=False))\n",
    "\n",
    "\n",
    "# Drop any duplicate row\n",
    "before = len(df)\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "print(\"\\nDuplicate rows removed\")\n",
    "print(before - len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f907a033",
   "metadata": {},
   "source": [
    "Insights:\n",
    "\n",
    "I have futher processed the categorical features to prepare them before i train my models. \n",
    "\n",
    "For the Category feature, it was encoded using the integer encoding, where each unique vehicle category was mapped to a numerical value such as 'Cabriolet'as 0, 'Coupe' as 1. This is to help keep the category information while also allowing the feature to be used in the models that does require a numeric inputs. \n",
    "\n",
    "While there are some binary categorical features that were converted into numerical form for simplicity. Such as Leather interior feature, it has been encoded so that \"yes\" as 1 and \"no\" as 0, while for the Wheel feature it was encoded to distinguish the \"left-hand drive\" as 0 and the \"right-hand drive\" as 1. The value counts for both before and after encoding are there to ensure that the original distribution were preserved during the changes. \n",
    "\n",
    "For the Drive wheels, the textual variation such as the \"front\",\"rear\",\"4x4\" and \"4wd\" were standarised into a consistent labels instead which are \"fwd\",\"rwd\" and for both last two it would be \"awd\" respectively. So this help to reduce the inconsistency in naming while maintaining the original meaning of its feature. \n",
    "\n",
    "For the Doors feature, it was clean by extracting the numeric door count and grouping it into two different categories, those that has less than 4 doors which is \"le_4\", and those that has more than 4 door under \"gt_$\". This is to simplify the feature and reduce the number of rare categories.\n",
    "\n",
    "Lastly, there are duplicated rows which would be identified and remove to prevent any repeated records from being able to influence the model when its training. All this steps helps to ensure that the categorical features are consistenly encoded, and simplified when needed, and is ready for the modelling portion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aaa5696",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Columns and data types in df before feature selection\")\n",
    "print(df.dtypes)\n",
    "print(\"Columns list\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "# Target\n",
    "y = df[\"Price_clean\"].copy()\n",
    "\n",
    "# Columns to drop from modelling\n",
    "drop_cols = [\n",
    "    \"Price\",\n",
    "    \"Price_clean\",\n",
    "    \"Category\",\n",
    "    \"Leather interior\",\n",
    "    \"Wheel\",\n",
    "    \"Doors\"\n",
    "]\n",
    "\n",
    "print(\"Columns planned to be dropped from X\")\n",
    "print(drop_cols)\n",
    "\n",
    "# Build feature matrix\n",
    "X = df.drop(columns=drop_cols, errors=\"ignore\").copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8ce91d",
   "metadata": {},
   "source": [
    "Insights:\n",
    "\n",
    "Before training the model, the target variable was defined as Price_clean, and the dataset was use to based off to select the relevant features. The original Price and Price_clean columns were then removed from the feature set to prevent any data leakage. \n",
    "\n",
    "In the original categorical columns such as Category, Leather interior, Wheel and Doors will be dropped as well as they had already been transformed into either encoded or as grouped features which means they have different column names. But the remaining of the dataset contains only the cleaned numerical and encoded categorical features, making it more suitable for model training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c26ca3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove ID from modelling if any was present\n",
    "if \"ID\" in X.columns:\n",
    "    X = X.drop(columns=[\"ID\"], errors=\"ignore\")\n",
    "    print(\"ID column removed from X\")\n",
    "\n",
    "print(\"Columns and data types in X before one hot encoding\")\n",
    "print(X.dtypes)\n",
    "\n",
    "print(\"Columns remaining in X before one hot encoding\")\n",
    "print(X.columns.tolist())\n",
    "\n",
    "# One hot encode the remaining categorical columns\n",
    "X = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "# Convert bool to int although it may be optional but it can help some model to be cleaner\n",
    "for c in X.columns:\n",
    "    if X[c].dtype == bool:\n",
    "        X[c] = X[c].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4e4056",
   "metadata": {},
   "source": [
    "Insights:\n",
    "\n",
    "Before doing the encoding, I had to check the remaining features and their datatypes again to identify which is the categorical columns. But for assurance, I have added the \"ID\" columns removal in case I had missed out previously again as I need to prevent it from influencing the model, since it dosent contian any predictive information.\n",
    "\n",
    "Currently the remaining categorical features, such as the Fuel type, Gear box type, Drive wheels and the Doors_group, were converted into numerical form using the One-hot encoding (OHE). So this allows the model to be properly interpret as a categorical data. The \"drop_first=True\" option is used to help with reducing the redundancy in the encoded features.\n",
    "\n",
    "Lastly, if there are any boolean values that were produced during encoding would be converted into integers to ensure that it is compatible with the machine learning models. After all these steps, the feature matric would only consist of entirely numerical values and it would be ready for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3e2c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values\n",
    "X = X.fillna(0)\n",
    "\n",
    "print(\"Columns and data types in X after one hot encoding\")\n",
    "print(X.dtypes)\n",
    "\n",
    "print(\"Columns used for modelling after encoding\")\n",
    "print(X.columns.tolist())\n",
    "\n",
    "print(\"Final feature matrix shape\")\n",
    "print(X.shape)\n",
    "\n",
    "print(\"Target shape\")\n",
    "print(y.shape)\n",
    "\n",
    "print(\"Count of data types in X\")\n",
    "print(X.dtypes.value_counts())\n",
    "\n",
    "print(\"Remaining object or category columns in X\")\n",
    "print(X.select_dtypes(include=[\"object\", \"category\"]).columns.tolist())\n",
    "\n",
    "print(\"Columns used for training\")\n",
    "print(X.columns.tolist())\n",
    "\n",
    "# save to clean csv to see the final feature matrix\n",
    "X.to_csv(\"cleaned_X.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd03f9f4",
   "metadata": {},
   "source": [
    "Insights:\n",
    "\n",
    "After I have done the One-hot encoding (OHE), all the remaining categorical features has been successfully converted into numerical form, and if there were any missing values it would have jus tbeen replaced with zeros. For the final last check to confirm there weren't any remaining object or categorical columns in the feature matric.But only for both Levy and Engine volume feature, they are stored as float values as they at first has non-integer values and were handled using numeric conversion and median imputations. Hence using floating point data type would help them to display the decimal values more accurately and to ensure that its compatible with the machine learning models.\n",
    "\n",
    "The final feature matric now has 15617 records and 21 feautres, although the target variable still has the same number of records, but now the dataset has mix of integer and float feature, which are suitable for the machine learning models and this helps me to confirm that it is fully prepared with consistent feature types and there are no missing values and is the model is ready to be trained."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf18b17",
   "metadata": {},
   "source": [
    "## 3.2 Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3823a1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14971b0a",
   "metadata": {},
   "source": [
    "Insights:\n",
    "\n",
    "For my dataset, I have split the training and the testing sets using 80/20 split to evaluate the model performance fairly. \n",
    "\n",
    "So the training set would contain at least 12493 rows and 21 features to be used to train the model while the test would contian 3124 rows and 21 features to be kept aside for the final evaluation. \n",
    "\n",
    "To ensure that the split is reproducible, there is a fixed \"random_state=42\", which means that if I were to rerun my code it would still obtaine the same results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f44f05",
   "metadata": {},
   "source": [
    "# 4. Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6268ad",
   "metadata": {},
   "source": [
    "### 4.2 Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b009f763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models\n",
    "lr_model = LinearRegression()\n",
    "\n",
    "dt_model = DecisionTreeRegressor()\n",
    "\n",
    "\n",
    "rf_model = RandomForestRegressor()\n",
    "\n",
    "# Gradient Boosting\n",
    "gbr_model = GradientBoostingRegressor()\n",
    "\n",
    "# Fit\n",
    "lr_model.fit(X_train, y_train)\n",
    "dt_model.fit(X_train, y_train)\n",
    "rf_model.fit(X_train, y_train)\n",
    "gbr_model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Models trained successfully\")\n",
    "print(\"- Linear Regression\")\n",
    "print(\"- Decision Tree\")\n",
    "print(\"- Random Forest\")\n",
    "print(\"- Gradient Boosting\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5814e80c",
   "metadata": {},
   "source": [
    "Insights:\n",
    "\n",
    "I have decided to use different regression models to train as I wanted to compare their performance based on the same dataset. The models are Linear Regression, Decision Tree, Random Forest and Gradient Boosting, which represents both simple and a more advance approaches. \n",
    "\n",
    "All the models were trained using the same training data to ensure that it is a fair comparison and that it will be easier to view their strength and weakness later based on the same test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a684e94",
   "metadata": {},
   "source": [
    "# 5. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1938206",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_reg(model, X_te, y_te):\n",
    "    pred = model.predict(X_te)\n",
    "    mae = mean_absolute_error(y_te, pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_te, pred))\n",
    "    r2 = r2_score(y_te, pred)\n",
    "    return pred, mae, rmse, r2\n",
    "\n",
    "\n",
    "models = {\n",
    "    \"Linear Regression\": lr_model,\n",
    "    \"Decision Tree Regressor\": dt_model,\n",
    "    \"Random Forest Regressor\": rf_model,\n",
    "    \"Gradient Boosting Regressor\": gbr_model\n",
    "}\n",
    "\n",
    "results = []\n",
    "preds = {}\n",
    "\n",
    "for name, m in models.items():\n",
    "    pred, mae, rmse, r2 = eval_reg(m, X_test, y_test)\n",
    "    preds[name] = pred\n",
    "    results.append({\"Model\": name, \"MAE\": mae, \"RMSE\": rmse, \"R2\": r2})\n",
    "\n",
    "results_df = pd.DataFrame(results).sort_values(by=\"R2\", ascending=False)\n",
    "print(\"Model Evaluation sorted by R2\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2dda9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To compare between the MAE and RMSE together\n",
    "df_plot = results_df.sort_values(\"R2\", ascending=False).copy()\n",
    "\n",
    "x = np.arange(len(df_plot))\n",
    "w = 0.35\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.bar(x - w/2, df_plot[\"MAE\"], width=w, label=\"MAE\")\n",
    "plt.bar(x + w/2, df_plot[\"RMSE\"], width=w, label=\"RMSE\")\n",
    "plt.xticks(x, df_plot[\"Model\"], rotation=20, ha=\"right\")\n",
    "plt.title(\"Model Comparison (Lower is better)\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9ce231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The R2 chart\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.bar(df_plot[\"Model\"], df_plot[\"R2\"])\n",
    "plt.title(\"Model Comparison - R2 (Higher is better)\")\n",
    "plt.xticks(rotation=20, ha=\"right\")\n",
    "plt.ylabel(\"R2\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b410be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get a visual for each of the model performance \n",
    "for name in df_plot[\"Model\"]:\n",
    "    pred = preds[name]\n",
    "    residuals = y_test - pred\n",
    "\n",
    "    # Actual vs Predicted\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.scatter(y_test, pred, alpha=0.5)\n",
    "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], linestyle=\"--\")\n",
    "    plt.title(f\"Actual vs Predicted - {name}\")\n",
    "    plt.xlabel(\"Actual Price_clean\")\n",
    "    plt.ylabel(\"Predicted Price_clean\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Residual plot\n",
    "    plt.figure(figsize=(7, 4))\n",
    "    plt.scatter(pred, residuals, alpha=0.5)\n",
    "    plt.axhline(0, linestyle=\"--\")\n",
    "    plt.title(f\"Residual Plot - {name}\")\n",
    "    plt.xlabel(\"Predicted Price_clean\")\n",
    "    plt.ylabel(\"Residual (Actual - Predicted)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34204f8f",
   "metadata": {},
   "source": [
    "explain why the number is too low??\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b30c5e",
   "metadata": {},
   "source": [
    "## Iterative model development\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd04c80",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dff3776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering using cleaned dataset from mode evaluation\n",
    "# Using the baseline Random Forest results from model evaluation to reused for comparison\n",
    "\n",
    "df_fe = df.copy()\n",
    "\n",
    "# Create car age from production year\n",
    "df_fe[\"Car_Age\"] = 2026 - df_fe[\"Prod. year\"]\n",
    "\n",
    "# Log transform mileage to reduce skew\n",
    "df_fe[\"Mileage_log\"] = np.log1p(df_fe[\"Mileage\"])\n",
    "\n",
    "# Interaction feature combining engine size and age\n",
    "df_fe[\"Engine_per_Age\"] = df_fe[\"Engine volume\"] / (df_fe[\"Car_Age\"] + 1)\n",
    "\n",
    "# Target variable\n",
    "y_fe = df_fe[\"Price_clean\"].copy()\n",
    "\n",
    "# Drop target and unused columns\n",
    "drop_cols = [\n",
    "    \"Price\",\n",
    "    \"Price_clean\",\n",
    "    \"Prod. year\",\n",
    "    \"Mileage\"\n",
    "]\n",
    "\n",
    "X_fe = df_fe.drop(columns=drop_cols, errors=\"ignore\")\n",
    "\n",
    "# One hot encode remaining categorical variables\n",
    "X_fe = pd.get_dummies(X_fe, drop_first=True)\n",
    "\n",
    "# Ensure no missing values remain\n",
    "X_fe = X_fe.fillna(0)\n",
    "\n",
    "# Train test split using same random state\n",
    "X_train_fe, X_test_fe, y_train_fe, y_test_fe = train_test_split(\n",
    "    X_fe,\n",
    "    y_fe,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train Random Forest on engineered features\n",
    "rf_fe = RandomForestRegressor(\n",
    "    n_estimators=200,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "rf_fe.fit(X_train_fe, y_train_fe)\n",
    "pred_fe = rf_fe.predict(X_test_fe)\n",
    "\n",
    "# Evaluate feature engineered model\n",
    "mae_fe = mean_absolute_error(y_test_fe, pred_fe)\n",
    "rmse_fe = np.sqrt(mean_squared_error(y_test_fe, pred_fe))\n",
    "r2_fe = r2_score(y_test_fe, pred_fe)\n",
    "\n",
    "print(\"Random Forest after Feature Engineering\")\n",
    "print(\"MAE:\", round(mae_fe, 2))\n",
    "print(\"RMSE:\", round(rmse_fe, 2))\n",
    "print(\"R2:\", round(r2_fe, 4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10776551",
   "metadata": {},
   "source": [
    "## HyperParameter Tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0867e175",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(title, y_true, y_pred):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "    print(title)\n",
    "    print(\"MAE:\", round(mae, 2))\n",
    "    print(\"RMSE:\", round(rmse, 2))\n",
    "    print(\"R2:\", round(r2, 4))\n",
    "    print()\n",
    "    return mae, rmse, r2\n",
    "\n",
    "# Baseline engineered model results for comparison\n",
    "pred_fe = rf_fe.predict(X_test_fe)\n",
    "mae_fe, rmse_fe, r2_fe = print_metrics(\n",
    "    \"Random Forest after Feature Engineering\",\n",
    "    y_test,\n",
    "    pred_fe\n",
    ")\n",
    "\n",
    "# Hyperparameter tuning using the engineered model as the estimator\n",
    "param_grid = {\n",
    "    \"n_estimators\": [120, 150, 200],\n",
    "    \"max_depth\": [15, 20, 35],\n",
    "    \"min_samples_split\": [4, 5, 6],\n",
    "    \"min_samples_leaf\": [1, 2, 3],\n",
    "    \"max_features\": [\"log2\", \"sqrt\"]\n",
    "}\n",
    "\n",
    "cv = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "search = RandomizedSearchCV(\n",
    "    estimator=rf_fe,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=20,\n",
    "    cv=cv,\n",
    "    scoring=\"r2\",\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "search.fit(X_train_fe, y_train)\n",
    "\n",
    "rf_tuned = search.best_estimator_\n",
    "pred_tuned = rf_tuned.predict(X_test_fe)\n",
    "\n",
    "mae_tuned, rmse_tuned, r2_tuned = print_metrics(\n",
    "    \"Random Forest after Hyperparameter Tuning\",\n",
    "    y_test,\n",
    "    pred_tuned\n",
    ")\n",
    "\n",
    "print(\"Best parameters found\")\n",
    "print(search.best_params_)\n",
    "print()\n",
    "\n",
    "final_model = rf_tuned\n",
    "final_stage = \"Hyperparameter Tuning\"\n",
    "if r2_tuned < r2_fe:\n",
    "    final_model = rf_fe\n",
    "    final_stage = \"Feature Engineering\"\n",
    "\n",
    "print(\"Final model selected stage:\", final_stage)\n",
    "print(\"Final R2:\", round(max(r2_fe, r2_tuned), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b278ca5",
   "metadata": {},
   "source": [
    "## Streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace33339",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(final_model, \"best_car_price_model.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mldp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
